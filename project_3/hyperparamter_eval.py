# -*- coding: utf-8 -*-
"""hyperparamter_eval.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1yvFR0PbjLzII6ismzT9rg13RW1x7_rsT
"""

import numpy as np
import pandas as pd
from sklearn.decomposition import NMF
from sklearn.preprocessing import normalize
import matplotlib.pyplot as plt

H = pd.read_csv("H_K5.csv", index_col=0).values
W = pd.read_csv("W_K5.csv", index_col=0).values
mut_matrix = np.dot(W, H)  # Reconstructed input matrix

def run_nmf(mut_matrix, n_components, init_method='nndsvd', solver='cd', beta_loss='frobenius', random_state=None):
    # Adjust parameters for KL divergence to avoid zero-update issues
    if beta_loss != 'frobenius':
        solver = 'mu'
        if init_method == 'nndsvd':
            init_method = 'nndsvda'  # safer choice with 'mu'

    model = NMF(n_components=n_components,
                init=init_method,
                solver=solver,
                beta_loss=beta_loss,
                max_iter=1000,
                random_state=random_state)
    W = model.fit_transform(mut_matrix)
    H = model.components_
    reconstruction = np.dot(W, H)
    error = np.linalg.norm(mut_matrix - reconstruction, 'fro')
    return error

def evaluate_initializations(mut_matrix, n_components=5, repeats=5):
    print("\nEvaluating different initializations:")
    results = {}
    for init in ['random', 'nndsvd', 'nndsvda', 'nndsvdar']:
        errors = []
        for i in range(repeats):
            error = run_nmf(mut_matrix, n_components, init_method=init, random_state=i)
            errors.append(error)
        mean_err = np.mean(errors)
        std_err = np.std(errors)
        results[init] = (mean_err, std_err)
        print(f"{init:12s} -> Mean error: {mean_err:.4f}, Std dev: {std_err:.4f}")
    return results

def evaluate_normalizations(mut_matrix, n_components=5, repeats=5):
    print("\nEvaluating different normalizations:")
    results = {}
    normalizations = {
        'none': mut_matrix,
        'row_norm': normalize(mut_matrix, norm='l1', axis=1),
        'col_norm': normalize(mut_matrix, norm='l1', axis=0)
    }
    for norm_name, matrix in normalizations.items():
        errors = []
        for i in range(repeats):
            error = run_nmf(matrix, n_components, random_state=i)
            errors.append(error)
        mean_err = np.mean(errors)
        std_err = np.std(errors)
        results[norm_name] = (mean_err, std_err)
        print(f"{norm_name:10s} -> Mean error: {mean_err:.4f}, Std dev: {std_err:.4f}")
    return results

def evaluate_objective_functions(mut_matrix, n_components=5, repeats=5):
    print("\nEvaluating different objective functions:")
    results = {}
    for loss in ['frobenius', 'kullback-leibler']:
        errors = []
        for i in range(repeats):
            error = run_nmf(mut_matrix, n_components, beta_loss=loss, random_state=i)
            errors.append(error)
        mean_err = np.mean(errors)
        std_err = np.std(errors)
        results[loss] = (mean_err, std_err)
        print(f"{loss:15s} -> Mean error: {mean_err:.4f}, Std dev: {std_err:.4f}")
    return results

def evaluate_optimal_k(mut_matrix, k_range=range(2, 11), repeats=5):
    print("\nEvaluating optimal number of components (k):")
    k_errors = {}
    for k in k_range:
        errors = []
        for i in range(repeats):
            error = run_nmf(mut_matrix, n_components=k, init_method='nndsvda', beta_loss='frobenius', random_state=i)
            errors.append(error)
        mean_err = np.mean(errors)
        std_err = np.std(errors)
        k_errors[k] = (mean_err, std_err)
        print(f"k={k:2d} -> Mean error: {mean_err:.4f}, Std dev: {std_err:.4f}")
    return k_errors

def plot_results(results, title):
    labels = list(results.keys())
    means = [results[k][0] for k in labels]
    stds = [results[k][1] for k in labels]

    plt.figure(figsize=(8, 5))
    plt.bar(labels, means, yerr=stds, capsize=5)
    plt.ylabel('Reconstruction Error (Frobenius norm)')
    plt.title(title)
    plt.xticks(rotation=45)
    plt.tight_layout()
    plt.show()

results_init = evaluate_initializations(mut_matrix)
results_norm = evaluate_normalizations(mut_matrix)
results_obj = evaluate_objective_functions(mut_matrix)
k_scores = evaluate_optimal_k(mut_matrix, k_range=range(2, 11), repeats=5)

plot_results(results_init, 'Initialization Methods Evaluation')
plot_results(results_norm, 'Normalization Strategies Evaluation')
plot_results(results_obj, 'Objective Functions Evaluation')
plot_results(k_scores, 'Optimal Number of Components (k) Evaluation')

import numpy as np
import pandas as pd
from sklearn.decomposition import NMF
from sklearn.preprocessing import normalize
from sklearn.metrics import pairwise_distances, silhouette_score, adjusted_rand_score
from sklearn.cluster import KMeans
from scipy.cluster.hierarchy import linkage, cophenet
from scipy.spatial.distance import squareform
import matplotlib.pyplot as plt

H = pd.read_csv("H_K5.csv", index_col=0).values
W = pd.read_csv("W_K5.csv", index_col=0).values
mut_matrix = np.dot(W, H)


def run_nmf(mut_matrix, n_components, init_method='nndsvd', solver='cd', beta_loss='frobenius', random_state=None):
    if beta_loss != 'frobenius':
        solver = 'mu'
        if init_method == 'nndsvd':
            init_method = 'nndsvda'

    model = NMF(n_components=n_components,
                init=init_method,
                solver=solver,
                beta_loss=beta_loss,
                max_iter=1000,
                random_state=random_state)
    W = model.fit_transform(mut_matrix)
    H = model.components_
    reconstruction = np.dot(W, H)
    error = np.linalg.norm(mut_matrix - reconstruction, 'fro')
    return error, W

# ============================
# Advanced k Evaluation (with Silhouette & ARI)
# ============================
def evaluate_k_with_stability(mut_matrix, k_range=range(2, 11), repeats=5):
    print("\nEvaluating k with reconstruction error, cophenetic correlation, silhouette score, and ARI")
    row_norm_matrix = normalize(mut_matrix, norm='l1', axis=1)
    results = {}

    for k in k_range:
        errors = []
        silhouettes = []
        aris = []
        Ws = []
        cluster_labels_all = []

        for i in range(repeats):
            error, W = run_nmf(row_norm_matrix, n_components=k, init_method='nndsvda', beta_loss='frobenius', random_state=i)
            errors.append(error)
            Ws.append(W)

            kmeans = KMeans(n_clusters=k, random_state=i).fit(W)
            labels = kmeans.labels_
            cluster_labels_all.append(labels)

            if W.shape[0] > k:
                silhouettes.append(silhouette_score(W, labels))

        # Compute average ARI over all pairs of runs
        pairwise_ari = []
        for i in range(repeats):
            for j in range(i + 1, repeats):
                pairwise_ari.append(adjusted_rand_score(cluster_labels_all[i], cluster_labels_all[j]))

        mean_err = np.mean(errors)
        mean_sil = np.mean(silhouettes) if silhouettes else 0
        mean_ari = np.mean(pairwise_ari) if pairwise_ari else 0

        # Cophenetic correlation
        W_concat = np.vstack(Ws)
        dist_matrix = pairwise_distances(W_concat)
        condensed_dist = squareform(dist_matrix, checks=False)
        linkage_matrix = linkage(condensed_dist, method='average')
        cophenetic_corr, _ = cophenet(linkage_matrix, condensed_dist)

        results[k] = {
            'error': mean_err,
            'cophenetic': cophenetic_corr,
            'silhouette': mean_sil,
            'ari': mean_ari
        }

        print(f"k={k:2d} -> Error: {mean_err:.4f}, Cophenetic: {cophenetic_corr:.4f}, Silhouette: {mean_sil:.4f}, ARI: {mean_ari:.4f}")

    return results

def plot_advanced_metrics(results):
    ks = list(results.keys())
    errors = [results[k]['error'] for k in ks]
    cophenetics = [results[k]['cophenetic'] for k in ks]
    silhouettes = [results[k]['silhouette'] for k in ks]
    aris = [results[k]['ari'] for k in ks]

    fig, axs = plt.subplots(2, 2, figsize=(12, 8))

    axs[0, 0].plot(ks, errors, marker='o')
    axs[0, 0].set_title('Reconstruction Error')

    axs[0, 1].plot(ks, cophenetics, marker='o')
    axs[0, 1].set_title('Cophenetic Correlation')

    axs[1, 0].plot(ks, silhouettes, marker='o')
    axs[1, 0].set_title('Silhouette Score')

    axs[1, 1].plot(ks, aris, marker='o')
    axs[1, 1].set_title('Adjusted Rand Index')

    for ax in axs.flat:
        ax.set_xlabel('Number of Components (k)')
        ax.grid(True)

    plt.tight_layout()
    plt.show()

advanced_k_results = evaluate_k_with_stability(mut_matrix, k_range=range(2, 11), repeats=5)
plot_advanced_metrics(advanced_k_results)

